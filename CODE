# ============================================================
# FULL IMPLEMENTATION: MA-DRL Scheduler with CTDE and Sensitivity-Aware Scheduling
# ============================================================

import os, glob, random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import xml.etree.ElementTree as ET
from dataclasses import dataclass
from collections import deque, namedtuple

# -------------------- Reproducibility --------------------
SEED = 0
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ============================================================
# 0) GLOBAL CONSTANTS (ALL DEFINED HERE -> NO NameError)
# ============================================================
USE_COMM = True

# RL
GAMMA = 0.95
LR = 1e-3
BATCH_SIZE = 128
REPLAY_CAP = 60000
MIN_REPLAY_SIZE = 3000
TARGET_UPDATE = 300
GRAD_CLIP = 1.0

# Budgets
DEADLINE_FACTOR = 1.5
ENERGY_BUDGET_FACTOR = 1.5

# Adaptive weights
W_LR = 0.08                 # weight update rate
SENS_LAMBDA = 0.35          # sensitivity emphasis
EPS = 1e-9

# Training
EPISODES_PER_WF = 250       # increase for stronger results
EPS_START = 0.9
EPS_END = 0.05

# ============================================================
# 1) Data Structures
# ============================================================
@dataclass
class Task:
    id: str
    runtime: float
    parents: list
    children: list
    data: float = 0.0

@dataclass
class DAG:
    tasks: dict
    topo_order: list

@dataclass
class Node:
    name: str
    mips: float
    cost: float
    qActive: float
    net_bw: float

# ============================================================
# 2) XML/DAX Parser (robust-ish)
# ============================================================
def parse_dax(path):
    tree = ET.parse(path)
    root = tree.getroot()

    def strip(tag):
        return tag.split('}')[-1] if '}' in tag else tag

    tasks = {}
    # jobs
    for el in root.iter():
        if strip(el.tag) == "job":
            tid = el.attrib.get("id") or el.attrib.get("name")
            if tid is None:
                continue
            rt = float(el.attrib.get("runtime", "0"))
            tasks[tid] = Task(tid, rt, [], [], 0.0)

            # input uses (optional)
            for u in el:
                if strip(u.tag) == "uses":
                    if u.attrib.get("link", "").lower() == "input":
                        tasks[tid].data += float(u.attrib.get("size", "0"))

    # edges
    for el in root.iter():
        if strip(el.tag) == "child":
            c = el.attrib.get("ref")
            if c not in tasks:
                continue
            for p in el:
                if strip(p.tag) == "parent":
                    pr = p.attrib.get("ref")
                    if pr in tasks:
                        tasks[c].parents.append(pr)
                        tasks[pr].children.append(c)

    # topo sort
    indeg = {t: len(tasks[t].parents) for t in tasks}
    q = deque([t for t in indeg if indeg[t] == 0])
    topo = []
    while q:
        u = q.popleft()
        topo.append(u)
        for v in tasks[u].children:
            indeg[v] -= 1
            if indeg[v] == 0:
                q.append(v)

    return DAG(tasks, topo)

# ============================================================
# 3) Scheduling core (exec/comm/EST) + LBs + metrics
# ============================================================
def exec_time(task: Task, node: Node, ref=1000.0):
    return task.runtime * (ref / (node.mips + EPS))

def comm_time(data, bw):
    if (not USE_COMM) or bw <= 0:
        return 0.0
    return data / bw

def comm_ready(dag: DAG, tid: str, node_name: str, schedule, nodes):
    parents = dag.tasks[tid].parents
    if not parents:
        return 0.0
    node_map = {n.name: n for n in nodes}
    per_parent_data = dag.tasks[tid].data / max(1, len(parents))
    ready = 0.0
    for p in parents:
        if p not in schedule:
            continue
        pn, _, pft = schedule[p]
        if pn == node_name:
            ready = max(ready, pft)
        else:
            bw = min(node_map[pn].net_bw, node_map[node_name].net_bw)
            ready = max(ready, pft + comm_time(per_parent_data, bw))
    return ready

def compute_LBs(dag: DAG, nodes):
    fastest = max(nodes, key=lambda n: n.mips)
    cheapest = min(nodes, key=lambda n: n.cost)
    lowestP = min(nodes, key=lambda n: n.qActive)

    et_fast = {t: exec_time(dag.tasks[t], fastest) for t in dag.tasks}

    dp = {}
    for t in dag.topo_order:
        if not dag.tasks[t].parents:
            dp[t] = et_fast[t]
        else:
            dp[t] = max(dp[p] for p in dag.tasks[t].parents) + et_fast[t]
    LB_T = max(dp.values()) if dp else 1.0
    LB_C = sum(exec_time(dag.tasks[t], cheapest) * cheapest.cost for t in dag.tasks)
    LB_E = sum(exec_time(dag.tasks[t], lowestP) * lowestP.qActive for t in dag.tasks)

    return max(LB_T, 1e-6), max(LB_C, 1e-6), max(LB_E, 1e-6)

# ============================================================
# 4) Features + System-aware Sensitivity Model
# ============================================================
def build_task_features(dag: DAG):
    # critical depth
    depth = {}
    for t in reversed(dag.topo_order):
        if not dag.tasks[t].children:
            depth[t] = dag.tasks[t].runtime
        else:
            depth[t] = dag.tasks[t].runtime + max(depth[c] for c in dag.tasks[t].children)
    maxd = max(depth.values()) if depth else 1.0

    feat = {}
    for t in dag.tasks:
        feat[t] = np.array([
            float(dag.tasks[t].runtime),
            float(dag.tasks[t].data),
            float(len(dag.tasks[t].parents)),
            float(len(dag.tasks[t].children)),
            float(depth[t] / (maxd + EPS))
        ], dtype=np.float32)
    return feat

def system_features(node_time, nodes, total_energy, energy_budget, deadline):
    loads = list(node_time.values())
    max_load = max(loads) if loads else 0.0
    avg_load = sum(loads) / max(1, len(loads))

    load_avg_norm = avg_load / (deadline + EPS)
    load_max_norm = max_load / (deadline + EPS)

    max_bw = max([n.net_bw for n in nodes]) if nodes else 1.0
    avg_bw = sum([n.net_bw for n in nodes]) / max(1, len(nodes))
    bw_norm = avg_bw / (max_bw + EPS)

    energy_pressure = total_energy / (energy_budget + EPS)
    energy_pressure = float(min(5.0, energy_pressure))

    return np.array([load_avg_norm, load_max_norm, bw_norm, energy_pressure], dtype=np.float32)

class SensitivityNet(nn.Module):
    """
    System-aware sensitivity: s = sigmoid( f([task_feat, sys_feat]) )
    """
    def __init__(self, in_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 64), nn.ReLU(),
            nn.Linear(64, 32), nn.ReLU(),
            nn.Linear(32, 1)
        )
    def forward(self, x):
        return torch.sigmoid(self.net(x))

def sensitivity_prioritize_order(dag: DAG, task_feat, sens_net, sys_feat_np):
    """
    ready-list scheduling:
      score = 0.55*critical_depth + 0.35*sensitivity + 0.10*runtime_term
    sensitivity depends on current system state too (sys_feat).
    """
    indeg = {t: len(dag.tasks[t].parents) for t in dag.tasks}
    ready = [t for t, d in indeg.items() if d == 0]
    order = []

    while ready:
        scores = []
        for t in ready:
            x = np.concatenate([task_feat[t], sys_feat_np], axis=0).astype(np.float32)
            with torch.no_grad():
                s = sens_net(torch.tensor(x).to(device)).item()
            cd = float(task_feat[t][-1])
            rt = float(task_feat[t][0])
            score = 0.55*cd + 0.35*s + 0.10*(rt/(1.0+rt))
            scores.append(score)
        idx = int(np.argmax(scores))
        u = ready.pop(idx)
        order.append(u)
        for v in dag.tasks[u].children:
            indeg[v] -= 1
            if indeg[v] == 0:
                ready.append(v)

    return order

# ============================================================
# 5) CTDE QMIX (standard form)
#   - each agent outputs Q_i(local_state, action) for actions {0,1}
#   - joint action = choose exactly one node => action vector has one "1"
#   - centralized critic: Q_tot = mix(q_i(a_i), global_state)
# ============================================================
class AgentQ(nn.Module):
    def __init__(self, in_dim, hidden=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden), nn.ReLU(),
            nn.Linear(hidden, hidden), nn.ReLU(),
            nn.Linear(hidden, 2)  # Q for a=0 and a=1
        )
    def forward(self, x):
        return self.net(x)

class QMixer(nn.Module):
    """
    Monotonic mixing network like QMIX:
    Q_tot = f(q_1..q_n, g_state) with non-negative weights
    """
    def __init__(self, n_agents, g_dim, hidden=64):
        super().__init__()
        self.n_agents = n_agents
        self.g_dim = g_dim
        self.hyper_w1 = nn.Sequential(nn.Linear(g_dim, hidden), nn.ReLU(), nn.Linear(hidden, n_agents))
        self.hyper_b1 = nn.Sequential(nn.Linear(g_dim, hidden), nn.ReLU(), nn.Linear(hidden, 1))
        self.hyper_w2 = nn.Sequential(nn.Linear(g_dim, hidden), nn.ReLU(), nn.Linear(hidden, 1))
        self.hyper_b2 = nn.Sequential(nn.Linear(g_dim, hidden), nn.ReLU(), nn.Linear(hidden, 1))

    def forward(self, q_agents, g_state):
        # q_agents: (B, n_agents)  chosen action-value per agent
        # g_state:  (B, g_dim)
        w1 = torch.abs(self.hyper_w1(g_state)) + EPS  # (B, n_agents)
        b1 = self.hyper_b1(g_state)                   # (B, 1)
        x = (q_agents * w1).sum(dim=1, keepdim=True) + b1
        w2 = torch.abs(self.hyper_w2(g_state)) + EPS  # (B, 1)
        b2 = self.hyper_b2(g_state)                   # (B, 1)
        y = x * w2 + b2
        return y.squeeze(1)                           # (B,)

Transition = namedtuple("Transition", ["g", "l", "a", "r", "ng", "nl", "done"])

class Replay:
    def __init__(self, cap):
        self.buf = deque(maxlen=cap)
    def push(self, *args):
        self.buf.append(Transition(*args))
    def sample(self, bs):
        batch = random.sample(self.buf, bs)
        return Transition(*zip(*batch))
    def __len__(self):
        return len(self.buf)

# ============================================================
# 6) Adaptive weights (Sensitivity-aware) + hook for exact paper formula
# ============================================================
def normalize_w(w):
    w = np.maximum(w, 0.01)
    return w / (w.sum() + EPS)

def update_weights_adaptive(w, obj_vec, sens_val):
    """
    Sensitivity-aware adaptive update.
    If you have an exact formula from the paper, replace THIS function body
    with the paper equation. Everything else will remain compatible.
    """
    v = np.array(obj_vec, dtype=np.float32)
    v_center = v - float(v.mean())
    w_new = w + W_LR * (1.0 + SENS_LAMBDA * sens_val) * v_center
    return normalize_w(w_new)

# ============================================================
# 7) Build states (global + local)
# ============================================================
def build_global_state(task_feat_vec, sys_feat_vec, w_vec, progress, dmr_ratio):
    # includes REAL DMR ratio so critic learns it too
    return np.concatenate([task_feat_vec, sys_feat_vec, w_vec, np.array([progress, dmr_ratio], dtype=np.float32)], axis=0)

def build_local_state(task_feat_vec, sys_feat_vec, node: Node, node_time, deadline, w_vec):
    # local includes node characteristics + queue + task + system + weights
    node_feat = np.array([
        node.mips / (node.mips + 2000.0),
        node.cost / (node.cost + 1.0),
        node.qActive / (node.qActive + 100.0),
        node.net_bw / (node.net_bw + 1e7),
        node_time[node.name] / (deadline + EPS)   # queue/load on this node
    ], dtype=np.float32)
    return np.concatenate([task_feat_vec, sys_feat_vec, node_feat, w_vec], axis=0)

# ============================================================
# 8) QMIX Trainer (CTDE)
# ============================================================
class QMIXTrainer:
    def __init__(self, nodes, g_dim, l_dim):
        self.nodes = nodes
        self.n = len(nodes)

        self.agents = nn.ModuleList([AgentQ(l_dim).to(device) for _ in range(self.n)])
        self.t_agents = nn.ModuleList([AgentQ(l_dim).to(device) for _ in range(self.n)])
        for i in range(self.n):
            self.t_agents[i].load_state_dict(self.agents[i].state_dict())

        self.mixer = QMixer(self.n, g_dim).to(device)
        self.t_mixer = QMixer(self.n, g_dim).to(device)
        self.t_mixer.load_state_dict(self.mixer.state_dict())

        self.opt = optim.Adam(list(self.agents.parameters()) + list(self.mixer.parameters()), lr=LR)
        self.replay = Replay(REPLAY_CAP)
        self.step = 0

    @torch.no_grad()
    def select_node(self, local_states, eps):
        # local_states: (n_agents, l_dim)
        if random.random() < eps:
            return random.randrange(self.n)
        # choose node by Q_i(a=1)
        qs = []
        for i in range(self.n):
            x = torch.tensor(local_states[i]).unsqueeze(0).float().to(device)
            q = self.agents[i](x)[0, 1].item()
            qs.append(q)
        return int(np.argmax(qs))

    def optimize(self):
        if len(self.replay) < MIN_REPLAY_SIZE:
            return

        batch = self.replay.sample(BATCH_SIZE)

        g = torch.tensor(np.stack(batch.g)).float().to(device)                      # (B, g_dim)
        l = torch.tensor(np.stack([np.stack(x) for x in batch.l])).float().to(device)  # (B, n, l_dim)
        a = torch.tensor(batch.a).long().to(device)                                 # (B,)
        r = torch.tensor(batch.r).float().to(device)                                # (B,)
        done = torch.tensor(batch.done).float().to(device)                          # (B,)

        # current q_agents chosen by joint action vector
        # we define joint action as: selected agent has a_i=1, others a_i=0
        q_chosen = []
        for i in range(self.n):
            qi_all = self.agents[i](l[:, i, :])  # (B, 2)
            # choose 1 if i==a else 0
            ai = (a == i).long()  # (B,)
            qi = qi_all.gather(1, ai.unsqueeze(1)).squeeze(1)  # (B,)
            q_chosen.append(qi)
        q_chosen = torch.stack(q_chosen, dim=1)  # (B, n)
        q_tot = self.mixer(q_chosen, g)          # (B,)

        # target
        next_q = torch.zeros_like(r)
        non_final_mask = torch.tensor([ng is not None for ng in batch.ng], dtype=torch.bool, device=device)
        if non_final_mask.any():
            ng = torch.tensor(np.stack([ng for ng in batch.ng if ng is not None])).float().to(device)
            nl = torch.tensor(np.stack([np.stack(ls) for ls, ngv in zip(batch.nl, batch.ng) if ngv is not None])).float().to(device)

            # greedy next action by max Q_i(a=1) from target agents (decentralized execution)
            q1s = []
            for i in range(self.n):
                qi = self.t_agents[i](nl[:, i, :])[:, 1]  # (B_nf,)
                q1s.append(qi)
            q1s = torch.stack(q1s, dim=1)                 # (B_nf, n)
            next_a = torch.argmax(q1s, dim=1)             # (B_nf,)

            # build q_chosen_next according to next_a
            q_chosen_next = []
            for i in range(self.n):
                qi_all = self.t_agents[i](nl[:, i, :])    # (B_nf, 2)
                ai = (next_a == i).long()                 # (B_nf,)
                qi = qi_all.gather(1, ai.unsqueeze(1)).squeeze(1)
                q_chosen_next.append(qi)
            q_chosen_next = torch.stack(q_chosen_next, dim=1)
            q_tot_next = self.t_mixer(q_chosen_next, ng)

            next_q[non_final_mask] = q_tot_next

        target = r + GAMMA * (1.0 - done) * next_q
        loss = nn.MSELoss()(q_tot, target.detach())

        self.opt.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(list(self.agents.parameters()) + list(self.mixer.parameters()), GRAD_CLIP)
        self.opt.step()

        self.step += 1
        if self.step % TARGET_UPDATE == 0:
            for i in range(self.n):
                self.t_agents[i].load_state_dict(self.agents[i].state_dict())
            self.t_mixer.load_state_dict(self.mixer.state_dict())

# ============================================================
# 9) One training episode on a workflow
# ============================================================
def run_episode_train_with_eps(dag: DAG, nodes, trainer: QMIXTrainer, sens_net: SensitivityNet, sens_opt, w, LB_T, LB_C, LB_E, eps):
    deadline = DEADLINE_FACTOR * LB_T
    energy_budget = ENERGY_BUDGET_FACTOR * LB_E

    task_feat = build_task_features(dag)

    node_time = {n.name: 0.0 for n in nodes}
    schedule = {}
    finish = {}

    total_cost = 0.0
    total_energy = 0.0

    miss_count = 0

    sysf0 = system_features(node_time, nodes, total_energy, energy_budget, deadline)
    order = sensitivity_prioritize_order(dag, task_feat, sens_net, sysf0)

    n_tasks = len(order)
    if n_tasks == 0:
        return w

    for k, tid in enumerate(order):
        progress = k / max(1, n_tasks)
        dmr_ratio = miss_count / max(1, k) if k > 0 else 0.0

        sysf = system_features(node_time, nodes, total_energy, energy_budget, deadline)
        tf = task_feat[tid]

        # sensitivity
        x_sens = np.concatenate([tf, sysf], axis=0).astype(np.float32)
        s_tensor = torch.tensor(x_sens).to(device)
        s_val = sens_net(s_tensor).item()

        g = build_global_state(tf, sysf, w, progress, dmr_ratio)

        local_states = []
        for n in nodes:
            local_states.append(build_local_state(tf, sysf, n, node_time, deadline, w))
        local_states = np.stack(local_states, axis=0)

        # choose node (decentralized exec)
        a = trainer.select_node(local_states, eps=eps)
        chosen = nodes[a]

        # simulate assignment
        est = max(node_time[chosen.name], comm_ready(dag, tid, chosen.name, schedule, nodes))
        et = exec_time(dag.tasks[tid], chosen)
        ft = est + et

        schedule[tid] = (chosen.name, est, ft)
        finish[tid] = ft
        node_time[chosen.name] = ft

        incC = et * chosen.cost
        incE = et * chosen.qActive
        total_cost += incC
        total_energy += incE

        # REAL miss update + REAL DMR ratio (ratio of missed tasks so far)
        miss = 1.0 if ft > deadline else 0.0
        miss_count += int(miss)
        dmr_ratio_next = miss_count / (k + 1)

        # normalized objectives (instant)
        t_norm = et / LB_T
        c_norm = incC / LB_C
        e_norm = incE / LB_E
        d_norm = dmr_ratio_next  # <<< REAL DMR ratio drives learning, not only binary

        # reward uses sensitivity amplification (paper concept)
        scalar = (w[0]*t_norm + w[1]*c_norm + w[2]*e_norm + w[3]*d_norm) * (1.0 + SENS_LAMBDA*s_val)
        reward = -float(scalar)

        # next state
        if k == n_tasks - 1:
            ng = None
            nl = None
            done = True
        else:
            next_tid = order[k+1]
            sysf2 = system_features(node_time, nodes, total_energy, energy_budget, deadline)
            tf2 = task_feat[next_tid]
            x_sens2 = np.concatenate([tf2, sysf2], axis=0).astype(np.float32)
            progress2 = (k+1)/max(1, n_tasks)
            ng = build_global_state(tf2, sysf2, w, progress2, dmr_ratio_next)
            nl_list = []
            for n in nodes:
                nl_list.append(build_local_state(tf2, sysf2, n, node_time, deadline, w))
            nl = np.stack(nl_list, axis=0)
            done = False

        trainer.replay.push(g, local_states, a, reward, ng, nl, done)
        trainer.optimize()

        # train sensitivity net online:
        harm = float(min(1.0, max(0.0, (-reward)/3.0)))
        s_pred = sens_net(s_tensor)
        loss_s = (s_pred - torch.tensor([harm], device=device))**2
        sens_opt.zero_grad()
        loss_s.mean().backward()
        sens_opt.step()

        # adaptive weights update (sensitivity-aware)
        w = update_weights_adaptive(w, [t_norm, c_norm, e_norm, d_norm], sens_val=s_val)

    return w, schedule

# ============================================================
# 10) RUN ALL WORKFLOWS
# ============================================================
# --- Define your nodes (replace with your PCS values if needed)
NODES = [
    Node("Fog1", 2000, 0.02, 60, 8e6),
    Node("Fog2", 2500, 0.03, 70, 1e7),
    Node("Fog3", 1800, 0.018, 55, 7e6),
    Node("Cloud1", 8000, 0.08, 140, 2e7),
]

# --- Set XML folder
XML_DIR = "/content"   # <-- change if needed (e.g., /content/drive/MyDrive/Colab_XMLs)

xmls = sorted(glob.glob(os.path.join(XML_DIR, "*.xml")))
print("XML files found:", len(xmls))
if len(xmls) == 0:
    print("WARNING: no XML found. Upload XML files or set XML_DIR correctly.")

results = []

for path in xmls:
    wf_name = os.path.splitext(os.path.basename(path))[0]
    dag = parse_dax(path)
    if dag is None or not dag.tasks:
        print("Skipping invalid:", wf_name)
        continue

    LB_T, LB_C, LB_E = compute_LBs(dag, NODES)

    # dims
    task_feat = build_task_features(dag)
    tf_dim = len(next(iter(task_feat.values())))
    sys_dim = 4
    w_dim = 4
    # global: task + sys + w + progress + dmr_ratio
    g_dim = tf_dim + sys_dim + w_dim + 2
    # local: task + sys + node_feat(5) + w
    l_dim = tf_dim + sys_dim + 5 + w_dim

    # models
    trainer = QMIXTrainer(NODES, g_dim=g_dim, l_dim=l_dim)
    sens_net = SensitivityNet(in_dim=tf_dim + sys_dim).to(device)
    sens_opt = optim.Adam(sens_net.parameters(), lr=1e-3)

    # init weights
    w = normalize_w(np.array([0.6, 0.1, 0.1, 0.2], dtype=np.float32))

    # train
    for ep in range(EPISODES_PER_WF):
        eps = EPS_START + (EPS_END - EPS_START) * (ep / max(1, EPISODES_PER_WF - 1))
        w, _ = run_episode_train_with_eps(dag, NODES, trainer, sens_net, sens_opt, w, LB_T, LB_C, LB_E, eps)

    # final greedy schedule
    sched = schedule_greedy(dag, NODES, trainer, sens_net, w, LB_T, LB_C, LB_E)
    ev = evaluate_schedule(dag, NODES, sched, LB_T, LB_C, LB_E)

    print(f"[{wf_name}] T={ev['T']:.2f} C={ev['C']:.2f} E={ev['E']:.2f} DMR={ev['DMR']:.3f} QoS={ev['QoS']:.4f} | w={w}")

    results.append({
        "Workflow": wf_name,
        "Makespan": ev["T"],
        "Cost": ev["C"],
        "Energy": ev["E"],
        "DMR": ev["DMR"],
        "QoS": ev["QoS"],
        "SLR": ev["SLR"],
        "CR": ev["CR"],
        "ER": ev["ER"],
        "w_time": float(w[0]),
        "w_cost": float(w[1]),
        "w_energy": float(w[2]),
        "w_deadline": float(w[3]),
    })

if results:
    df = pd.DataFrame(results).sort_values("Workflow")
    print("\n==============================")
    print(" FINAL RESULTS (QMIX + Sensitivity + Real DMR)")
    print("==============================\n")
    print(df[["Workflow", "Makespan", "Cost", "Energy", "DMR", "QoS"]].to_string(index=False))

    print("\n=== Averages ===")
    for col in ["Makespan", "Cost", "Energy", "DMR", "QoS", "SLR", "CR", "ER"]:
        print(f"{col}: {df[col].mean():.4f}")
